{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\Documents\\master\\ProGen\\progen\\progen2\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "\n",
    "#from lib.model.progen.init_model import init_model\n",
    "from lib.model.progen.extended.init_model import init_model\n",
    "from lib.model.train import train\n",
    "from lib.model.progen.init_tokenizer import init_tokenizer\n",
    "from lib.data.datasets.GB1 import get_GB1_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = init_tokenizer()\n",
    "tokenize = lambda sequence: torch.tensor(tokenizer.encode(sequence).ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at s:\\Documents\\master\\code\\llm\\lib\\model\\progen/../../../../../ProGen/progen/progen2/checkpoints/progen2-small were not used when initializing ProGenModel: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing ProGenModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ProGenModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = init_model().to(device)\n",
    "#model = init_model(state_dict_path=\"/models/progen_extended_0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([param for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save / Load model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.getcwd() + \"/models/progen_extended_0.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, MODEL_PATH)\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load(MODEL_PATH)\n",
    "\n",
    "# ! init the model first\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\Documents\\master\\ProGen\\progen\\progen2\\.venv\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "TEST_SPLIT = 0.1\n",
    "\n",
    "tokenize = lambda sequence: tokenizer.encode(sequence).ids\n",
    "\n",
    "train_sequences, train_fitnesses, test_sequences, test_fitnesses = get_GB1_dataset(\n",
    "    tokenize=tokenize,\n",
    "    test_split=TEST_SPLIT,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DATA = 100\n",
    "\n",
    "n_train = int(N_DATA * (1-TEST_SPLIT))\n",
    "n_test = int(N_DATA * TEST_SPLIT)\n",
    "train_sequences = train_sequences[0:n_train]\n",
    "train_fitnesses = train_fitnesses[0:n_train]\n",
    "test_sequences = test_sequences[0:n_test]\n",
    "test_fitnesses = test_fitnesses[0:n_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([56, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.forward(train_sequences[0])\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = train(\n",
    "    model=model,\n",
    "    device=device,\n",
    "\n",
    "    train_data=train_sequences,\n",
    "    train_labels=train_fitnesses,\n",
    "    test_data=test_sequences,\n",
    "    test_labels=test_fitnesses,\n",
    "\n",
    "    loss_function=torch.nn.functional.mse_loss,\n",
    "    batch_size=1,\n",
    "    learning_rate=1e-3,\n",
    "    n_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained 0.9 data, tested on 0.1 data, shuffled\n",
    "\n",
    "mean test loss = 1.2047\n",
    "mean test loss = 0.3296"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained on batches of size 1, tested on 0.1 data, shuffled\n",
    "\n",
    "mean test-data loss = 2.927136182785034\n",
    "s:\\Documents\\master\\code\\llm\\lib\\model\\train.py:44: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "  loss = loss_function(output, label)\n",
    "mean test-data loss = 1.0202242136001587\n",
    "Epoch 1:mean test-data loss = 0.7981275320053101\n",
    "Epoch 2:mean test-data loss = 0.8598976135253906\n",
    "Epoch 3:mean test-data loss = 0.8339541554450989\n",
    "Epoch 4:mean test-data loss = 0.2656804621219635\n",
    "Epoch 5:mean test-data loss = 0.22678536176681519\n",
    "Epoch 6:mean test-data loss = 0.7484277486801147\n",
    "Epoch 7:mean test-data loss = 0.7273379564285278\n",
    "Epoch 8:mean test-data loss = 0.20019400119781494\n",
    "Epoch 9:mean test-data loss = 0.2083941400051117\n",
    "Epoch 10:mean test-data loss = 0.49960991740226746\n",
    "Epoch 11:mean test-data loss = 0.4153096675872803\n",
    "Epoch 12:mean test-data loss = 0.30266302824020386\n",
    "Epoch 13:mean test-data loss = 0.3114781379699707\n",
    "Epoch 14:mean test-data loss = 0.29903849959373474\n",
    "Epoch 15:mean test-data loss = 0.2538512945175171\n",
    "Epoch 16:mean test-data loss = 0.4101998507976532\n",
    "Epoch 17:mean test-data loss = 0.41078630089759827\n",
    "Epoch 18:mean test-data loss = 0.2096560150384903\n",
    "Epoch 19:mean test-data loss = 0.19284088909626007\n",
    "Epoch 20:mean test-data loss = 0.4778718650341034\n",
    "Epoch 21:mean test-data loss = 0.4716760814189911\n",
    "Epoch 22:mean test-data loss = 0.18762770295143127\n",
    "Epoch 23:mean test-data loss = 0.8848859071731567\n",
    "Epoch 24:mean test-data loss = 1.1906050443649292\n",
    "Epoch 25:mean test-data loss = 1.0409295558929443\n",
    "Epoch 26:mean test-data loss = 0.4904017448425293\n",
    "Epoch 27:mean test-data loss = 0.5127981901168823\n",
    "Epoch 28:mean test-data loss = 0.8855107426643372\n",
    "Epoch 29:mean test-data loss = 0.8350266218185425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Progen Extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\Documents\\master\\ProGen\\progen\\progen2\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from progen_extended import train_progen_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to device\n",
      "Loading tokenizer\n",
      "Loading GB1 data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\Documents\\master\\ProGen\\progen\\progen2\\.venv\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at s:\\Documents\\master\\code\\llm\\lib\\model\\progen/../../../../../ProGen/progen/progen2/checkpoints/progen2-small were not used when initializing ProGenModel: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing ProGenModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ProGenModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 9.57 GiB (GPU 0; 8.00 GiB total capacity; 7.02 GiB already allocated; 0 bytes free; 7.02 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32ms:\\Documents\\master\\code\\llm\\progen_extended.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/s%3A/Documents/master/code/llm/progen_extended.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loss_history \u001b[39m=\u001b[39m train_progen_extended(\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/Documents/master/code/llm/progen_extended.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     state_dict_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/Documents/master/code/llm/progen_extended.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     save_state_dict\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/models/progen_extended_tmp.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/Documents/master/code/llm/progen_extended.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     save_history\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/models/progen_extended_tmp_history.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/Documents/master/code/llm/progen_extended.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n",
      "File \u001b[1;32ms:\\Documents\\master\\code\\llm\\progen_extended.py:59\u001b[0m, in \u001b[0;36mtrain_progen_extended\u001b[1;34m(learning_rate, loss_function, test_split, batch_size, n_epochs, state_dict_path, save_state_dict, save_history, absolute_paths)\u001b[0m\n\u001b[0;32m     53\u001b[0m model \u001b[39m=\u001b[39m init_model(\n\u001b[0;32m     54\u001b[0m     state_dict_path\u001b[39m=\u001b[39mstate_dict_path,\n\u001b[0;32m     55\u001b[0m     absolute_path\u001b[39m=\u001b[39mabsolute_paths,\n\u001b[0;32m     56\u001b[0m )\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     58\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m loss_history \u001b[39m=\u001b[39m train(\n\u001b[0;32m     60\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     61\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m     train_data\u001b[39m=\u001b[39;49mtrain_sequences,\n\u001b[0;32m     64\u001b[0m     train_labels\u001b[39m=\u001b[39;49mtrain_fitnesses,\n\u001b[0;32m     65\u001b[0m     test_data\u001b[39m=\u001b[39;49mtest_sequences,\n\u001b[0;32m     66\u001b[0m     test_labels\u001b[39m=\u001b[39;49mtest_fitnesses,\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m     loss_function\u001b[39m=\u001b[39;49mloss_function,\n\u001b[0;32m     69\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m     70\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     71\u001b[0m     n_epochs\u001b[39m=\u001b[39;49mn_epochs,\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m save_state_dict:\n\u001b[0;32m     75\u001b[0m     save_model_states(\n\u001b[0;32m     76\u001b[0m         model,\n\u001b[0;32m     77\u001b[0m         save_to\u001b[39m=\u001b[39msave_state_dict,\n\u001b[0;32m     78\u001b[0m         absolute_path\u001b[39m=\u001b[39mabsolute_paths,\n\u001b[0;32m     79\u001b[0m     )\n",
      "File \u001b[1;32ms:\\Documents\\master\\code\\llm\\lib\\model\\train.py:29\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_data, train_labels, test_data, test_labels, loss_function, batch_size, learning_rate, n_epochs, device)\u001b[0m\n\u001b[0;32m     26\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[0;32m     28\u001b[0m loss_history \u001b[39m=\u001b[39m init_loss_history(n_epochs, n_batches)\n\u001b[1;32m---> 29\u001b[0m loss_history[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m evaluation_step(\n\u001b[0;32m     30\u001b[0m     model, device, test_data, test_labels, loss_function\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[0;32m     33\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_batches):\n",
      "File \u001b[1;32ms:\\Documents\\master\\code\\llm\\lib\\model\\train.py:69\u001b[0m, in \u001b[0;36mevaluation_step\u001b[1;34m(model, device, test_data, test_labels, loss_function, epoch, batch)\u001b[0m\n\u001b[0;32m     67\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     68\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 69\u001b[0m     output \u001b[39m=\u001b[39m model(test_data)[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m]\n\u001b[0;32m     70\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(output, test_labels)\n\u001b[0;32m     71\u001b[0m     evaluation_print(loss, epoch, batch)\n",
      "File \u001b[1;32ms:\\Documents\\master\\ProGen\\progen\\progen2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32ms:\\Documents\\master\\code\\llm\\lib\\model\\progen\\extended\\TransformerExtended.py:25\u001b[0m, in \u001b[0;36mTransformerExtended.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m     12\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     13\u001b[0m     input_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m ):\n\u001b[1;32m---> 25\u001b[0m     transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m     26\u001b[0m         input_ids,\n\u001b[0;32m     27\u001b[0m         past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m     28\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m     29\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m     30\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m     31\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m     32\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m     33\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m     34\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m     35\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m     36\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m     37\u001b[0m     )\n\u001b[0;32m     38\u001b[0m     hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m     39\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfitness_head(hidden_states)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[1;32ms:\\Documents\\master\\ProGen\\progen\\progen2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32ms:\\Documents\\master\\code\\llm\\lib\\model\\progen/../../../../../ProGen\\progen\\progen2\\models\\progen\\modeling_progen.py:501\u001b[0m, in \u001b[0;36mProGenModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    493\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    494\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    495\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    498\u001b[0m         head_mask[i],\n\u001b[0;32m    499\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 501\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[0;32m    502\u001b[0m         hidden_states,\n\u001b[0;32m    503\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    504\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    505\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m    506\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    507\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    508\u001b[0m     )\n\u001b[0;32m    510\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    511\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32ms:\\Documents\\master\\ProGen\\progen\\progen2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32ms:\\Documents\\master\\code\\llm\\lib\\model\\progen/../../../../../ProGen\\progen\\progen2\\models\\progen\\modeling_progen.py:263\u001b[0m, in \u001b[0;36mProGenBlock.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    261\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m    262\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 263\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[0;32m    264\u001b[0m     hidden_states,\n\u001b[0;32m    265\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    266\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    267\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    268\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    269\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    270\u001b[0m )\n\u001b[0;32m    271\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    272\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32ms:\\Documents\\master\\ProGen\\progen\\progen2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32ms:\\Documents\\master\\code\\llm\\lib\\model\\progen/../../../../../ProGen\\progen\\progen2\\models\\progen\\modeling_progen.py:157\u001b[0m, in \u001b[0;36mProGenAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    148\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    149\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m ):\n\u001b[1;32m--> 157\u001b[0m     qkv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqkv_proj(hidden_states)\n\u001b[0;32m    158\u001b[0m     \u001b[39m# TODO(enijkamp): factor out number of logical TPU-v3/v4 cores or make forward pass agnostic\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[39m# mp_num = 4\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     mp_num \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m\n",
      "File \u001b[1;32ms:\\Documents\\master\\ProGen\\progen\\progen2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32ms:\\Documents\\master\\ProGen\\progen\\progen2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32ms:\\Documents\\master\\ProGen\\progen\\progen2\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n\u001b[0;32m   1846\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[1;32m-> 1847\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 9.57 GiB (GPU 0; 8.00 GiB total capacity; 7.02 GiB already allocated; 0 bytes free; 7.02 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "loss_history = train_progen_extended(\n",
    "    state_dict_path=\"\",\n",
    "    save_state_dict=\"/models/progen_extended_tmp.pt\",\n",
    "    save_history=\"/models/progen_extended_tmp_history.pt\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "progen_venv",
   "language": "python",
   "name": "progen_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
